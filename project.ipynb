{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMkw-0-_q04m",
        "colab_type": "code",
        "outputId": "16336644-8bbc-48e1-acc0-ad31c7aa6b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#https://blog.csdn.net/u013714645/article/details/97899342\n",
        "import pandas as pd\n",
        "from deepctr.inputs import SparseFeat, VarLenSparseFeat\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.models import Model\n",
        "from deepmatch.models import *\n",
        "from deepmatch.utils import sampledsoftmaxloss\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from sklearn.externals import joblib\n",
        "import os\n",
        "import random\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJmJtPlvQmsv",
        "colab_type": "code",
        "outputId": "3a3bda1e-9084-4966-bea2-e3a388068eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnloTGmhQ-HF",
        "colab_type": "code",
        "outputId": "99390c24-d683-424c-bc8c-fbb880cab21a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "! pip install deepmatch\n",
        "! pip install deepctr\n",
        "! pip install faiss-cpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepmatch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/0a/a0c3dc659a7eadc7f3606a7a1e9bc683be9605b0f015c11be4290a449755/deepmatch-0.1.3-py3-none-any.whl\n",
            "Collecting deepctr==0.7.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/84/c3baa57def7778ce2b297cdcb1d1daddc41e608efa2714c1862f3f7c861d/deepctr-0.7.5-py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from deepmatch) (2.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from deepmatch) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->deepmatch) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py->deepmatch) (1.18.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->deepmatch) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->deepmatch) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->deepmatch) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->deepmatch) (2020.4.5.1)\n",
            "Installing collected packages: deepctr, deepmatch\n",
            "Successfully installed deepctr-0.7.5 deepmatch-0.1.3\n",
            "Requirement already satisfied: deepctr in /usr/local/lib/python3.6/dist-packages (0.7.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from deepctr) (2.23.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from deepctr) (2.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr) (2.9)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py->deepctr) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->deepctr) (1.12.0)\n",
            "Collecting faiss-cpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/84/9de38703486d9f00b1a63590887a318d08c52f10f768968bd7626aee75da/faiss_cpu-1.6.3-cp36-cp36m-manylinux2010_x86_64.whl (7.2MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from faiss-cpu) (1.18.4)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09WfHp66rRbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget http://tianchi-public-us-east-download.oss-us-east-1.aliyuncs.com/231785/underexpose_train.zip -O /content/gdrive/My Drive/underexpose_train.zip \n",
        "! wget http://tianchi-public-us-east-download.oss-us-east-1.aliyuncs.com/231785/underexpose_test.zip -O /content/gdrive/My Drive/underexpose_test.zip\n",
        "! unzip -o /content/gdrive/My Drive/underexpose_train.zip  \n",
        "! unzip -o /content/gdrive/My Drive/underexpose_test.zip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buZF_IwZuZRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv \"underexpose_train/underexpose_train_click-0.csv\" \"/..\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c80PiwZjzYAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/gdrive/My Drive/project_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAX9i6T0znC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaD9dCpFztMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv 'underexpose_user_feat.csv' '/content'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXoRGsESwM0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!7z e -pdee22d5e4a7b1e3c409ea0719aa0a715 underexpose_test/underexpose_test_click-6.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xNiGNtMtqMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_data_set(data, negsample=0):\n",
        "\n",
        "    data.sort_values(\"timestamp\", inplace=True)\n",
        "    item_ids = data['movie_id'].unique()\n",
        "\n",
        "    train_set = []\n",
        "    test_set = []\n",
        "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
        "  \n",
        "        pos_list = hist['movie_id'].tolist()\n",
        "        rating_list = hist['rating'].tolist()\n",
        "\n",
        "        if negsample > 0:\n",
        "            candidate_set = list(set(item_ids) - set(pos_list))\n",
        "            neg_list = np.random.choice(candidate_set,size=len(pos_list)*negsample,replace=True)\n",
        "        for i in range(1, len(pos_list)):\n",
        "            hist = pos_list[:i]\n",
        "            if i != len(pos_list) - 1:\n",
        "                train_set.append((reviewerID, hist[::-1], pos_list[i], 1, len(hist[::-1]),rating_list[i]))\n",
        "                for negi in range(negsample):\n",
        "                    train_set.append((reviewerID, hist[::-1], neg_list[i*negsample+negi], 0,len(hist[::-1])))\n",
        "            else:\n",
        "                test_set.append((reviewerID, hist[::-1], pos_list[i], 1,len(hist[::-1]),rating_list[i]))\n",
        "\n",
        "    random.shuffle(train_set)\n",
        "    random.shuffle(test_set)\n",
        "\n",
        "    print(len(train_set[0]),len(test_set[0]))\n",
        "\n",
        "    return train_set,test_set\n",
        "def gen_model_input(train_set,user_profile,seq_max_len):\n",
        "\n",
        "    train_uid = np.array([line[0] for line in train_set])\n",
        "    train_seq = [line[1] for line in train_set]\n",
        "    train_iid = np.array([line[2] for line in train_set])\n",
        "    train_label = np.array([line[3] for line in train_set])\n",
        "    train_hist_len = np.array([line[4] for line in train_set])\n",
        "\n",
        "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
        "    train_model_input = {\"user_id\": train_uid, \"movie_id\": train_iid, \"hist_movie_id\": train_seq_pad,\n",
        "                         \"hist_len\": train_hist_len}\n",
        "\n",
        "    for key in [\"gender\", \"age\", \"occupation\"]:\n",
        "        train_model_input[key] = user_profile.loc[train_model_input['user_id']][key].values\n",
        "\n",
        "    return train_model_input, train_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUVltI2vtd0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_item_feat(path):\n",
        "    col_name = ['item_id']\n",
        "    for i in range(256):\n",
        "      col_name.append(str(i))\n",
        "    \n",
        "    item_feat = pd.read_csv(path, header=None, names=col_name)\n",
        "    \n",
        "    item_feat.iloc[:,   1] = [float(i) for i in item_feat.iloc[:,   1].str[1:]]\n",
        "    item_feat.iloc[:, 128] = [float(i) for i in item_feat.iloc[:, 128].str[:-1]]\n",
        "    item_feat.iloc[:, 129] = [float(i) for i in item_feat.iloc[:, 129].str[1:]]\n",
        "    item_feat.iloc[:, 256] = [float(i) for i in item_feat.iloc[:, 256].str[:-1]]\n",
        "    return item_feat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JKi9JaceNnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_item_cnt(df_, user_col, item_col):\n",
        "    sim_txt, sim_img = joblib.load(os.path.join(data_path+'sim_dict.pkl'))\n",
        "\n",
        "    df = df_.copy()\n",
        "    user_item_ = df.groupby(user_col)[item_col].agg(list).reset_index()\n",
        "    user_item_dict = dict(zip(user_item_[user_col], user_item_[item_col]))\n",
        "\n",
        "    rateing=list()\n",
        "    total = 0\n",
        "    item_cnt = defaultdict(int)  # 商品被点击次数\n",
        "    for user, items in tqdm(user_item_dict.items()):\n",
        "        for loc1, item in enumerate(items):\n",
        "            total += 1\n",
        "            item_cnt[item] += 1   \n",
        "            rate = 0\n",
        "            if item in sim_txt.keys() and item in sim_img.keys():\n",
        "                for ritem, sim in sim_txt[item].items():\n",
        "                    if ritem in items and ritem !=item:\n",
        "                        rate += sim\n",
        "                for ritem, sim in sim_img[item].items():   \n",
        "                    if ritem in items and ritem !=item:\n",
        "                        rate += sim\n",
        "                rate /= len(items*2)\n",
        "            rateing.append([user, item, rate])\n",
        "    for i, rlist in enumerate(rateing):\n",
        "        if rate == 0:\n",
        "            rateing[i] = [rlist[0], rlist[1], item_cnt[item]/total]\n",
        "    return pd.DataFrame([(a, b, c) for a, b, c in rateing], columns=['user_id', 'movie_id', 'rating'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5cvdQRAtTkO",
        "colab_type": "code",
        "outputId": "c471bc08-d2ef-457e-b1ea-d2863e32612d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_path = '/content/gdrive/My Drive/project_data/'\n",
        "now_phase = 6\n",
        "#item_feat = read_item_feat(data_path+'underexpose_item_feat.csv')\n",
        "user_feat = pd.read_csv(data_path+'underexpose_user_feat.csv', header=None, names=['user_id', 'age', 'gender', 'occupation'])\n",
        "user_feat['gender'] = (user_feat['gender'] != 'M').astype(int)\n",
        "\n",
        "test_click = pd.DataFrame()\n",
        "train_click = pd.DataFrame()\n",
        "for c in range(now_phase + 1):\n",
        "    test_tmp = pd.read_csv(data_path + '/underexpose_test_click-{}.csv'.format(c), header=None, names=['user_id', 'movie_id', 'timestamp'])\n",
        "    train_tmp = pd.read_csv(data_path + '/underexpose_train_click-{}.csv'.format(c), header=None, names=['user_id', 'movie_id', 'timestamp'])\n",
        "\n",
        "    test_click = test_click.append(test_tmp)\n",
        "    test_click = test_click.drop_duplicates(subset=['user_id', 'movie_id', 'timestamp'], keep='last')\n",
        "    train_click = train_click.append(test_tmp)\n",
        "    train_click = train_click.append(train_tmp)\n",
        "    train_click = train_click.drop_duplicates(subset=['user_id', 'movie_id', 'timestamp'], keep='last')\n",
        "    train_click = train_click.sort_values('timestamp')\n",
        "\n",
        "click_to_rating = get_item_cnt(train_click, 'user_id', 'movie_id')\n",
        "\n",
        "test_click = pd.merge(test_click, user_feat, how='left', on=['user_id'])\n",
        "train_click = pd.merge(train_click, user_feat, how='left', on=['user_id'])\n",
        "\n",
        "test_click = pd.merge(test_click, click_to_rating, how='left', on=['user_id','movie_id'])\n",
        "train_click = pd.merge(train_click, click_to_rating, how='left', on=['user_id','movie_id'])\n",
        "\n",
        "#test_click = pd.merge(test_click, item_feat, how='left', on=['item_id'])\n",
        "#train_click = pd.merge(train_click, item_feat, how='left', on=['item_id'])\n",
        "test_click = test_click.fillna(test_click.mean())\n",
        "train_click = train_click.fillna(train_click.mean())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31525/31525 [01:49<00:00, 288.10it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25r_rbeOtVF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "outputId": "5ee2983c-ca1e-4759-cec6-d6c025cac6ca"
      },
      "source": [
        "#data = pd.read_csvdata = pd.read_csv(\"./movielens_sample.txt\")\n",
        "data = train_click.copy()\n",
        "\n",
        "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", ]\n",
        "SEQ_LEN = 50\n",
        "negsample = 0\n",
        "\n",
        "# 1.Label Encoding for sparse features,and process sequence features with `gen_date_set` and `gen_model_input`\n",
        "labelencoder_dict = {}\n",
        "feature_max_idx = {}\n",
        "features = ['user_id', 'movie_id', 'gender', 'age', 'occupation']\n",
        "for feature in features:\n",
        "    lbe = LabelEncoder()\n",
        "    if feature == 'user_id' or feature == 'movie_id':\n",
        "        tmp_data = data[feature].copy()\n",
        "        data[feature] = lbe.fit_transform(data[feature])\n",
        "        feature_max_idx[feature] = data[feature].max() \n",
        "        labelencoder_dict[feature]=lbe\n",
        "        #　print(labelencoder_dict[feature].inverse_transform(data.head()[feature]))\n",
        "        \n",
        "    else:\n",
        "        data[feature] = lbe.fit_transform(data[feature]) +1\n",
        "        feature_max_idx[feature] = data[feature].max() +1\n",
        "\n",
        "user_profile = data[[\"user_id\", \"gender\", \"age\", \"occupation\"]].drop_duplicates('user_id')\n",
        "\n",
        "item_profile = data[[\"movie_id\"]].drop_duplicates('movie_id')\n",
        "\n",
        "\n",
        "user_profile.set_index(\"user_id\", inplace=True)\n",
        "\n",
        "user_item_list = data.groupby(\"user_id\")['movie_id'].apply(list)\n",
        "\n",
        "train_set, test_set= gen_data_set(data, negsample)\n",
        "\n",
        "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
        "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)\n",
        "\n",
        "# 2.count #unique features for each sparse field and generate feature config for sequence feature\n",
        "\n",
        "embedding_dim = 32\n",
        "\n",
        "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], 16),\n",
        "                        SparseFeat(\"gender\", feature_max_idx['gender'], 16),\n",
        "                        SparseFeat(\"age\", feature_max_idx['age'], 16),\n",
        "                        SparseFeat(\"occupation\", feature_max_idx['occupation'], 16),\n",
        "                        VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], embedding_dim,\n",
        "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
        "                        ]\n",
        "\n",
        "item_feature_columns = [SparseFeat('movie_id', feature_max_idx['movie_id'], embedding_dim)]\n",
        "\n",
        "# 3.Define Model and train\n",
        "\n",
        "K.set_learning_phase(True)\n",
        "\n",
        "import tensorflow as tf\n",
        "if tf.__version__ >= '2.0.0':\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "model = YoutubeDNN(user_feature_columns, item_feature_columns, num_sampled=100, user_dnn_hidden_units=(128,64, embedding_dim))\n",
        "# model = MIND(user_feature_columns,item_feature_columns,dynamic_k=False,p=1,k_max=2,num_sampled=100,user_dnn_hidden_units=(128,64, embedding_dim),init_std=0.001)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)  # \"binary_crossentropy\")\n",
        "\n",
        "history = model.fit(train_model_input, train_label,  # train_label,\n",
        "                    batch_size=512, epochs=20, verbose=1, validation_split=0.0, )\n",
        "\n",
        "# 4. Generate user features for testing and full item features for retrieval\n",
        "test_user_model_input = test_model_input\n",
        "all_item_model_input = {\"movie_id\": item_profile['movie_id'].values,}\n",
        "\n",
        "user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
        "item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
        "\n",
        "user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
        "# user_embs = user_embs[:, i, :]  # i in [0,k_max) if MIND\n",
        "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
        "\n",
        "print(user_embs.shape)\n",
        "print(item_embs.shape)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31525/31525 [00:14<00:00, 2246.73it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6 6\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 1057399 samples\n",
            "Epoch 1/20\n",
            "1057399/1057399 [==============================] - 243s 230us/sample - loss: 3.1099\n",
            "Epoch 2/20\n",
            "1057399/1057399 [==============================] - 239s 226us/sample - loss: 2.9326\n",
            "Epoch 3/20\n",
            "1057399/1057399 [==============================] - 241s 228us/sample - loss: 2.8797\n",
            "Epoch 4/20\n",
            "1057399/1057399 [==============================] - 239s 226us/sample - loss: 2.8336\n",
            "Epoch 5/20\n",
            "1057399/1057399 [==============================] - 240s 227us/sample - loss: 2.7580\n",
            "Epoch 6/20\n",
            "1057399/1057399 [==============================] - 240s 227us/sample - loss: 2.6534\n",
            "Epoch 7/20\n",
            "1057399/1057399 [==============================] - 242s 229us/sample - loss: 2.5641\n",
            "Epoch 8/20\n",
            "1057399/1057399 [==============================] - 240s 227us/sample - loss: 2.4860\n",
            "Epoch 9/20\n",
            "1057399/1057399 [==============================] - 240s 227us/sample - loss: 2.4240\n",
            "Epoch 10/20\n",
            "1057399/1057399 [==============================] - 239s 226us/sample - loss: 2.3618\n",
            "Epoch 11/20\n",
            "1057399/1057399 [==============================] - 240s 227us/sample - loss: 2.2986\n",
            "Epoch 12/20\n",
            "1057399/1057399 [==============================] - 239s 226us/sample - loss: 2.2541\n",
            "Epoch 13/20\n",
            "1057399/1057399 [==============================] - 245s 232us/sample - loss: 2.2133\n",
            "Epoch 14/20\n",
            "1057399/1057399 [==============================] - 240s 227us/sample - loss: 2.1803\n",
            "Epoch 15/20\n",
            "1057399/1057399 [==============================] - 243s 230us/sample - loss: 2.1406\n",
            "Epoch 16/20\n",
            "1057399/1057399 [==============================] - 243s 230us/sample - loss: 2.1016\n",
            "Epoch 17/20\n",
            "1057399/1057399 [==============================] - 241s 228us/sample - loss: 2.0954\n",
            "Epoch 18/20\n",
            "1057399/1057399 [==============================] - 243s 230us/sample - loss: 2.0729\n",
            "Epoch 19/20\n",
            "1057399/1057399 [==============================] - 245s 232us/sample - loss: 2.0468\n",
            "Epoch 20/20\n",
            "1057399/1057399 [==============================] - 246s 233us/sample - loss: 2.0356\n",
            "(31525, 32)\n",
            "(98769, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWfQuTBatZFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "66f1b11f-bef1-4ac6-b9a7-250ad3285e2a"
      },
      "source": [
        "test_true_label = {line[0]:[line[2]] for line in test_set}\n",
        "result = pd.DataFrame()\n",
        "import numpy as np\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from deepmatch.utils import recall_N\n",
        "result = {}\n",
        "index = faiss.IndexFlatIP(embedding_dim)\n",
        "# faiss.normalize_L2(item_embs)\n",
        "index.add(item_embs)\n",
        "# faiss.normalize_L2(user_embs)\n",
        "D, I = index.search(np.ascontiguousarray(user_embs), 50)\n",
        "s = []\n",
        "hit = 0\n",
        "for i, uid in tqdm(enumerate(test_user_model_input['user_id'])):\n",
        "    try:\n",
        "        pred = [item_profile['movie_id'].values[x] for x in I[i]]\n",
        "        filter_item = None\n",
        "        recall_score = recall_N(test_true_label[uid], pred, N=50)\n",
        "        s.append(recall_score)\n",
        "        if test_true_label[uid] in pred:\n",
        "            hit += 1\n",
        "        result.setdefault(uid, list())  \n",
        "        result[uid] = pred\n",
        "    except:\n",
        "        print(i)\n",
        "print(\"\")\n",
        "print(\"recall\", np.mean(s))\n",
        "print(\"hit rate\", hit / len(test_user_model_input['user_id']))\n",
        "joblib.dump(result, os.path.join(data_path+'result.pkl'))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31525it [00:07, 4019.30it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "recall 0.023409992069785883\n",
            "hit rate 0.023409992069785883\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/My Drive/project_data/result.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXCwYEHHoHHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(data_path+'underexpose_submit-{}.csv'.format(now_phase), 'w', newline='') as csvfile:\n",
        "  writer  = csv.writer(csvfile)\n",
        "  for k, v in result.items():\n",
        "      row = labelencoder_dict['user_id'].inverse_transform([k])\n",
        "      row = np.append(row, labelencoder_dict['movie_id'].inverse_transform(v), axis=0)\n",
        "      writer.writerow(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uUCdfcJATjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "col_name = ['user_id']\n",
        "for i in range(50):\n",
        "  col_name.append(str(i))\n",
        "a = pd.read_csv(data_path+'underexpose_submit-{}.csv'.format(now_phase), header=None, names=col_name)\n",
        "b = pd.DataFrame()\n",
        "for c in range(now_phase + 1):\n",
        "    test_tmp = pd.read_csv(data_path + 'underexpose_test_qtime-{}.csv'.format(c), header=None, names=['user_id', 'timestamp'])\n",
        "    b = b.append(test_tmp)\n",
        "    b = b.drop_duplicates(subset=['user_id', 'timestamp'], keep='last')\n",
        "\n",
        "print(a.head())\n",
        "print(b.head())\n",
        "b = pd.merge(b, a, how='left', on=['user_id'])\n",
        "b.drop('timestamp', axis=1, inplace=True)  \n",
        "b = b.drop_duplicates(subset=['user_id'], keep='last')\n",
        "print(b.head())\n",
        "b.to_csv(data_path + 'underexpose_submit-T.csv', index=False, header=None)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgcnyF2VHcuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}