{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMkw-0-_q04m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://blog.csdn.net/u013714645/article/details/97899342\n",
        "import pandas as pd\n",
        "from deepctr.inputs import SparseFeat, VarLenSparseFeat, DenseFeat\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.models import Model\n",
        "from deepmatch.models import *\n",
        "from deepmatch.utils import sampledsoftmaxloss\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from sklearn.externals import joblib\n",
        "import os\n",
        "import random\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJmJtPlvQmsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/gdrive') \n",
        "data_path = '/content/gdrive/My Drive/project_data/'\n",
        "now_phase = 6\n",
        "SEQ_LEN = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnloTGmhQ-HF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install deepmatch\n",
        "! pip install deepctr\n",
        "! pip install faiss-cpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09WfHp66rRbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget http://tianchi-public-us-east-download.oss-us-east-1.aliyuncs.com/231785/underexpose_train.zip -O /content/gdrive/My Drive/underexpose_train.zip \n",
        "! wget http://tianchi-public-us-east-download.oss-us-east-1.aliyuncs.com/231785/underexpose_test.zip -O /content/gdrive/My Drive/underexpose_test.zip\n",
        "! unzip -o /content/gdrive/My Drive/underexpose_train.zip  \n",
        "! unzip -o /content/gdrive/My Drive/underexpose_test.zip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c80PiwZjzYAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/gdrive/My Drive/project_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAX9i6T0znC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaD9dCpFztMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_item_feat(path):\n",
        "    col_name = ['movie_id']\n",
        "    for i in range(256):\n",
        "      col_name.append(str(i))\n",
        "    \n",
        "    item_feat = pd.read_csv(path, header=None, names=col_name)\n",
        "    \n",
        "    item_feat.iloc[:,   1] = [float(i) for i in item_feat.iloc[:,   1].str[1:]]\n",
        "    item_feat.iloc[:, 128] = [float(i) for i in item_feat.iloc[:, 128].str[:-1]]\n",
        "    item_feat.iloc[:, 129] = [float(i) for i in item_feat.iloc[:, 129].str[1:]]\n",
        "    item_feat.iloc[:, 256] = [float(i) for i in item_feat.iloc[:, 256].str[:-1]]\n",
        "    return item_feat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xNiGNtMtqMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_data_set(data):\n",
        "    data.sort_values(\"timestamp\", inplace=True)\n",
        "    item_ids = data['movie_id'].unique()\n",
        "\n",
        "    train_set = []\n",
        "    test_set = []\n",
        "\n",
        "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
        "        pos_list = hist['movie_id'].tolist()\n",
        "        gender_list = hist['gender'].tolist()\n",
        "        age_list = hist['age'].tolist()\n",
        "        occupation_list = hist['occupation'].tolist()\n",
        "        txt_feature = hist[[str(i) for i in range(0, 128)]].values.tolist()\n",
        "        #img_feature = hist[[str(i) for i in range(128, 256)]].values.tolist()\n",
        "        for i in range(1, len(pos_list)):\n",
        "            hist = pos_list[:i]   \n",
        "            if i != len(pos_list) - 1:\n",
        "                #train_set.append((reviewerID, hist[::-1], pos_list[i], 1, len(hist[::-1]),gender_list[i],age_list[i],occupation_list[i],txt_feature[i],img_feature[i]))\n",
        "                train_set.append([reviewerID, hist[::-1], pos_list[i], 1, len(hist[::-1]),gender_list[i],age_list[i],occupation_list[i],txt_feature[i]])\n",
        "\n",
        "            else:\n",
        "                #test_set.append((reviewerID, hist[::-1], pos_list[i], 1,len(hist[::-1]),gender_list[i],age_list[i],occupation_list[i],txt_feature[i],img_feature[i]))\n",
        "                test_set.append([reviewerID, hist[::-1], pos_list[i], 1,len(hist[::-1]),gender_list[i],age_list[i],occupation_list[i],txt_feature[i]])\n",
        "\n",
        "    random.shuffle(train_set)\n",
        "    random.shuffle(test_set)\n",
        "    return train_set,test_set\n",
        "def gen_model_input(train_set,seq_max_len):\n",
        "\n",
        "    train_uid = np.array([line[0] for line in train_set])\n",
        "    train_seq = [line[1] for line in train_set]\n",
        "    train_iid = np.array([line[2] for line in train_set])\n",
        "    train_label = np.array([line[3] for line in train_set])\n",
        "    train_hist_len = np.array([line[4] for line in train_set])\n",
        "    train_gender = np.array([line[5] for line in train_set])\n",
        "    train_age = np.array([line[6] for line in train_set])\n",
        "    train_occupation = np.array([line[7] for line in train_set])\n",
        "    train_txt = np.array([line[8] for line in train_set])\n",
        "    #train_img = np.array([line[9] for line in train_set])\n",
        "\n",
        "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
        "    train_model_input = {\"user_id\": train_uid, \"movie_id\": train_iid, \"hist_movie_id\": train_seq_pad,\n",
        "                         \"hist_len\": train_hist_len, \"gender\": train_gender, \"age\": train_age, \n",
        "                         \"occupation\": train_occupation, \"train_txt\": train_txt}#, \"train_img\": train_img}\n",
        "\n",
        "    return train_model_input, train_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5cvdQRAtTkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "item_feat = read_item_feat(data_path+'underexpose_item_feat.csv')\n",
        "user_feat = pd.read_csv(data_path+'underexpose_user_feat.csv', header=None, names=['user_id', 'age', 'gender', 'occupation'])\n",
        "user_feat['gender'] = (user_feat['gender'] != 'M').astype(int)\n",
        "\n",
        "test_click = pd.DataFrame()\n",
        "train_click = pd.DataFrame()\n",
        "for c in range(now_phase + 1):\n",
        "    test_tmp = pd.read_csv(data_path + '/underexpose_test_click-{}.csv'.format(c), header=None, names=['user_id', 'movie_id', 'timestamp'])\n",
        "    train_tmp = pd.read_csv(data_path + '/underexpose_train_click-{}.csv'.format(c), header=None, names=['user_id', 'movie_id', 'timestamp'])\n",
        "\n",
        "    test_click = test_click.append(test_tmp)\n",
        "    test_click = test_click.drop_duplicates(subset=['user_id', 'movie_id', 'timestamp'], keep='last')\n",
        "    train_click = train_click.append(test_tmp)\n",
        "    train_click = train_click.append(train_tmp)\n",
        "    train_click = train_click.drop_duplicates(subset=['user_id', 'movie_id', 'timestamp'], keep='last')\n",
        "    train_click = train_click.sort_values('timestamp')\n",
        "\n",
        "\n",
        "print('item_feat:', item_feat['movie_id'].nunique())\n",
        "print('item_total:', train_click['movie_id'].nunique())\n",
        "print('no item feat:',len(set(train_click['movie_id'].unique()).difference(set(item_feat['movie_id'].unique()))))\n",
        "\n",
        "test_click = pd.merge(test_click, user_feat, how='left', on=['user_id'])\n",
        "train_click = pd.merge(train_click, user_feat, how='left', on=['user_id'])\n",
        "\n",
        "#test_click = pd.merge(test_click, click_to_rating, how='left', on=['user_id','movie_id'])\n",
        "#train_click = pd.merge(train_click, click_to_rating, how='left', on=['user_id','movie_id'])\n",
        "\n",
        "test_click = test_click.sort_values(by=['user_id','timestamp'])\n",
        "train_click = train_click.sort_values(by=['user_id','timestamp'])\n",
        "\n",
        "test_click = pd.merge(test_click, item_feat, how='left', on=['movie_id'])\n",
        "train_click = pd.merge(train_click, item_feat, how='left', on=['movie_id'])\n",
        "\n",
        "test_click = test_click.fillna(method='bfill')\n",
        "train_click = train_click.fillna(method='bfill')\n",
        "joblib.dump(train_click, os.path.join(data_path+'train_click.pkl'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSShyTK0W4zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = joblib.load(os.path.join(data_path+'train_click.pkl'))\n",
        "# 1.Label Encoding for sparse features,and process sequence features with `gen_date_set` and `gen_model_input`\n",
        "labelencoder_dict = {}\n",
        "feature_max_idx = {}\n",
        "features = ['user_id', 'movie_id', 'gender', 'age', 'occupation']\n",
        "for feature in features:\n",
        "    lbe = LabelEncoder()\n",
        "    if feature == 'user_id' or feature == 'movie_id':\n",
        "        tmp_data = data[feature].copy()\n",
        "        data[feature] = lbe.fit_transform(data[feature])\n",
        "        feature_max_idx[feature] = data[feature].max() \n",
        "        labelencoder_dict[feature]=lbe\n",
        "        #　print(labelencoder_dict[feature].inverse_transform(data.head()[feature]))\n",
        "        \n",
        "    else:\n",
        "        data[feature] = lbe.fit_transform(data[feature]) \n",
        "        feature_max_idx[feature] = data[feature].max() \n",
        "\n",
        "item_profile = data[[\"movie_id\"]].drop_duplicates('movie_id')\n",
        "# user_profile = data[[\"user_id\", \"gender\", \"age\", \"occupation\"]].drop_duplicates('user_id')\n",
        "# user_profile.set_index(\"user_id\", inplace=True)\n",
        "# user_item_list = data.groupby(\"user_id\")['movie_id'].apply(list)\n",
        "\n",
        "train_set, test_set= gen_data_set(data)\n",
        "train_model_input, train_label = gen_model_input(train_set, SEQ_LEN)\n",
        "test_model_input, test_label = gen_model_input(test_set, SEQ_LEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25r_rbeOtVF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2.count #unique features for each sparse field and generate feature config for sequence feature\n",
        "\n",
        "embedding_dim = 32\n",
        "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], 16),\n",
        "                        SparseFeat(\"gender\", feature_max_idx['gender'], 16),\n",
        "                        SparseFeat(\"age\", feature_max_idx['age'], 16),\n",
        "                        SparseFeat(\"occupation\", feature_max_idx['occupation'], 16),\n",
        "                        DenseFeat(\"train_txt\", 128),\n",
        "                        #DenseFeat(\"train_img\", 128),\n",
        "                        VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], embedding_dim,\n",
        "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
        "                        ]\n",
        "\n",
        "item_feature_columns = [SparseFeat('movie_id', feature_max_idx['movie_id'], embedding_dim)]\n",
        "\n",
        "# 3.Define Model and train\n",
        "\n",
        "K.set_learning_phase(True)\n",
        "\n",
        "import tensorflow as tf\n",
        "if tf.__version__ >= '2.0.0':\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "model = YoutubeDNN(user_feature_columns, item_feature_columns, num_sampled=100, user_dnn_hidden_units=(128,64, embedding_dim))\n",
        "# model = MIND(user_feature_columns,item_feature_columns,dynamic_k=False,p=1,k_max=2,num_sampled=100,user_dnn_hidden_units=(128,64, embedding_dim),init_std=0.001)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)  # \"binary_crossentropy\")\n",
        "\n",
        "history = model.fit(train_model_input, train_label,  # train_label,\n",
        "                    batch_size=512, epochs=20, verbose=1, validation_split=0.0, )\n",
        "\n",
        "# 4. Generate user features for testing and full item features for retrieval\n",
        "test_user_model_input = test_model_input\n",
        "all_item_model_input = {\"movie_id\": item_profile['movie_id'].values,}\n",
        "\n",
        "user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
        "item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
        "\n",
        "user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
        "# user_embs = user_embs[:, i, :]  # i in [0,k_max) if MIND\n",
        "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
        "\n",
        "print(user_embs.shape)\n",
        "print(item_embs.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWfQuTBatZFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_true_label = {line[0]:[line[2]] for line in test_set}\n",
        "result = pd.DataFrame()\n",
        "import numpy as np\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from deepmatch.utils import recall_N\n",
        "result = {}\n",
        "index = faiss.IndexFlatIP(embedding_dim)\n",
        "# faiss.normalize_L2(item_embs)\n",
        "index.add(item_embs)\n",
        "# faiss.normalize_L2(user_embs)\n",
        "D, I = index.search(np.ascontiguousarray(user_embs), 50)\n",
        "s = []\n",
        "hit = 0\n",
        "for i, uid in tqdm(enumerate(test_user_model_input['user_id'])):\n",
        "    try:\n",
        "        pred = [item_profile['movie_id'].values[x] for x in I[i]]\n",
        "        filter_item = None\n",
        "        recall_score = recall_N(test_true_label[uid], pred, N=50)\n",
        "        s.append(recall_score)\n",
        "        if test_true_label[uid] in pred:\n",
        "            hit += 1\n",
        "        result.setdefault(uid, list())  \n",
        "        result[uid] = pred\n",
        "    except:\n",
        "        print(i)\n",
        "\n",
        "print(\"recall\", np.mean(s))\n",
        "print(\"hit rate\", hit / len(test_user_model_input['user_id']))\n",
        "joblib.dump(result, os.path.join(data_path+'result.pkl'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXCwYEHHoHHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(data_path+'underexpose_submit-{}.csv'.format(now_phase), 'w', newline='') as csvfile:\n",
        "  writer  = csv.writer(csvfile)\n",
        "  for k, v in result.items():\n",
        "      row = labelencoder_dict['user_id'].inverse_transform([k])\n",
        "      row = np.append(row, labelencoder_dict['movie_id'].inverse_transform(v), axis=0)\n",
        "      writer.writerow(row)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}